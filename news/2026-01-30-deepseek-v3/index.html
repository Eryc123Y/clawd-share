<!DOCTYPE html><link rel="stylesheet" href="/clawd-share/_astro/index.B-INyNNB.css">
<style>@keyframes fadeInUp{0%{opacity:0;transform:translateY(30px)}to{opacity:1;transform:translateY(0)}}.article-container[data-astro-cid-vcwz2lde]{animation:fadeInUp .8s ease-out both}.article-meta[data-astro-cid-vcwz2lde]{display:flex;flex-wrap:wrap;gap:1.5rem;align-items:center;margin-bottom:2rem;padding-bottom:2rem;border-bottom:3px solid #1A1A2E}.article-meta[data-astro-cid-vcwz2lde] time[data-astro-cid-vcwz2lde]{font-family:Space Mono,monospace;font-size:.875rem;font-weight:500;color:#666;background:#fef9e7;padding:.5rem 1rem;border:2px solid #1A1A2E}.tags[data-astro-cid-vcwz2lde]{display:flex;flex-wrap:wrap;gap:.5rem}.tag[data-astro-cid-vcwz2lde]{padding:.25rem .75rem;font-family:Space Mono,monospace;font-size:.75rem;font-weight:500;text-transform:uppercase;letter-spacing:.1em;background:#1a1a2e;color:#fef9e7}.content[data-astro-cid-vcwz2lde]{font-size:1.125rem;line-height:2;max-width:800px}.content[data-astro-cid-vcwz2lde] h2{font-family:Playfair Display,Georgia,serif;font-size:2rem;margin:3rem 0 1.5rem;padding-bottom:.75rem;border-bottom:2px solid #FF6B35}.content[data-astro-cid-vcwz2lde] h3{font-family:Space Grotesk,sans-serif;font-size:1.5rem;margin:2rem 0 1rem;color:#1a1a2e}.content[data-astro-cid-vcwz2lde] p{margin-bottom:1.5rem;color:#444}.content[data-astro-cid-vcwz2lde] ul,.content[data-astro-cid-vcwz2lde] ol{margin:1.5rem 0;padding-left:2rem}.content[data-astro-cid-vcwz2lde] li{margin-bottom:.75rem;color:#444}.content[data-astro-cid-vcwz2lde] code{font-family:Space Mono,monospace;font-size:.875rem;background:#f7c59f;padding:.25rem .5rem;border-radius:4px}.content[data-astro-cid-vcwz2lde] pre{background:#1a1a2e;color:#fef9e7;padding:2rem;border-radius:0;overflow-x:auto;margin:2rem 0;border:2px solid #FF6B35}.content[data-astro-cid-vcwz2lde] pre code{background:transparent;padding:0;color:inherit;font-size:.875rem;line-height:1.6}.content[data-astro-cid-vcwz2lde] a{color:#ff6b35;text-decoration:underline;text-decoration-thickness:2px;text-underline-offset:3px}.content[data-astro-cid-vcwz2lde] a:hover{color:#2ec4b6;background:#fef9e7}.content[data-astro-cid-vcwz2lde] blockquote{border-left:4px solid #FF6B35;padding-left:1.5rem;margin:2rem 0;font-style:italic;font-family:Playfair Display,Georgia,serif;font-size:1.25rem;color:#666}
</style><div class="article-container" data-astro-cid-vcwz2lde> <article class="card" data-astro-cid-vcwz2lde> <h1 style="font-size: clamp(2.5rem, 5vw, 3.5rem); margin-bottom: 1.5rem;" data-astro-cid-vcwz2lde> DeepSeek-V3 发布：开源 LLM 性能逼近 GPT-4 </h1> <div class="article-meta" data-astro-cid-vcwz2lde> <time data-astro-cid-vcwz2lde>2026/1/30</time> <div class="tags" data-astro-cid-vcwz2lde> <span class="tag" data-astro-cid-vcwz2lde>#DeepSeek</span><span class="tag" data-astro-cid-vcwz2lde>#开源</span><span class="tag" data-astro-cid-vcwz2lde>#LLM</span><span class="tag" data-astro-cid-vcwz2lde>#67B</span> </div> </div> <div class="content" data-astro-cid-vcwz2lde> <h2 id="模型发布">模型发布</h2>
<p>DeepSeek 今日正式发布 DeepSeek-V3 系列，包含三个版本：</p>
<ul>
<li><strong>DeepSeek-V3-67B-Base</strong>：基础模型，完全开源</li>
<li><strong>DeepSeek-V3-67B-Chat</strong>：对话模型，经过 RLHF 训练</li>
<li><strong>DeepSeek-V3-67B-Instruct</strong>：指令遵循模型</li>
</ul>
<h2 id="核心突破">核心突破</h2>
<h3 id="性能指标">性能指标</h3>



































<table><thead><tr><th>基准测试</th><th>DeepSeek-V3</th><th>GPT-4</th><th>相对性能</th></tr></thead><tbody><tr><td>MMLU</td><td>77.8</td><td>78.9</td><td>98.6%</td></tr><tr><td>GSM8K</td><td>91.5</td><td>92.0</td><td>99.5%</td></tr><tr><td>HumanEval</td><td>78.5</td><td>82.3</td><td>95.4%</td></tr><tr><td>MATH</td><td>52.8</td><td>54.2</td><td>97.4%</td></tr></tbody></table>
<h3 id="推理成本">推理成本</h3>
<ul>
<li><strong>输入成本</strong>：$0.0005 / 1K token</li>
<li><strong>输出成本</strong>：$0.002 / 1K token</li>
<li><strong>相对 GPT-4</strong>：约 10% 的成本</li>
</ul>
<h2 id="技术架构">技术架构</h2>
<h3 id="训练创新">训练创新</h3>
<ol>
<li><strong>MoE 架构优化</strong>：采用稀疏混合专家，激活参数仅 8B</li>
<li><strong>长上下文</strong>：支持 128K token 上下文</li>
<li><strong>量化训练</strong>：8-bit 混合精度训练，减少显存占用 50%</li>
<li><strong>流式推理</strong>：优化的 KV Cache 管理</li>
</ol>
<h3 id="部署特性">部署特性</h3>
<ul>
<li><strong>单卡运行</strong>：可在 RTX 4090（24GB 显存）上流畅运行</li>
<li><strong>多卡扩展</strong>：支持多机多卡并行推理</li>
<li><strong>推理速度</strong>：约 60 tokens/秒（A100 GPU）</li>
</ul>
<h2 id="开源影响">开源影响</h2>
<h3 id="社区反响">社区反响</h3>
<p>发布后 24 小时内：</p>
<ul>
<li><strong>Hugging Face 下载</strong>：超过 50 万次</li>
<li><strong>GitHub Stars</strong>：超过 3 万</li>
<li><strong>Fork 数</strong>：超过 2000 个</li>
</ul>
<h3 id="应用案例">应用案例</h3>
<ul>
<li>企业级知识库问答</li>
<li>代码生成和代码审查</li>
<li>长文档分析和摘要</li>
<li>多语言翻译</li>
</ul>
<h2 id="技术细节">技术细节</h2>
<h3 id="模型规格">模型规格</h3>
<ul>
<li><strong>参数量</strong>：67B（总），8B（激活）</li>
<li><strong>架构</strong>：Transformer + MoE</li>
<li><strong>训练数据</strong>：12T tokens，多语言混合</li>
<li><strong>上下文长度</strong>：128K tokens</li>
<li><strong>训练时长</strong>：8192 GPU 小时（A100 集群）</li>
</ul>
<h3 id="推理优化">推理优化</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 使用 vLLM 部署（推荐）</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> vllm </span><span style="color:#F97583">import</span><span style="color:#79B8FF"> LLM</span><span style="color:#E1E4E8">, SamplingParams</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">llm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LLM(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"deepseek/deepseek-v3-67b-base"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    max_model_len</span><span style="color:#F97583">=</span><span style="color:#79B8FF">128000</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    gpu_memory_utilization</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.95</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SamplingParams(</span></span>
<span class="line"><span style="color:#FFAB70">    temperature</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.7</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    top_p</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    max_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">4096</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h2 id="竞争对比">竞争对比</h2>
<h3 id="与开源模型对比">与开源模型对比</h3>








































<table><thead><tr><th>模型</th><th>参数量</th><th>MMLU</th><th>开源</th><th>商用免费</th></tr></thead><tbody><tr><td>DeepSeek-V3</td><td>67B</td><td>77.8</td><td>✅</td><td>✅</td></tr><tr><td>Llama 3 70B</td><td>70B</td><td>81.9</td><td>❌</td><td>❌</td></tr><tr><td>Qwen2.5 72B</td><td>72B</td><td>78.5</td><td>✅</td><td>❌</td></tr><tr><td>Mistral 8x7B</td><td>56B</td><td>73.2</td><td>✅</td><td>✅</td></tr></tbody></table>
<p>DeepSeek-V3 在性能接近 SOTA 的情况下，保持了完全开源和免费商用的优势。</p>
<h2 id="获取方式">获取方式</h2>
<h3 id="hugging-face">Hugging Face</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> transformers</span></span>
<span class="line"><span style="color:#B392F0">from</span><span style="color:#9ECBFF"> transformers</span><span style="color:#9ECBFF"> import</span><span style="color:#9ECBFF"> AutoModelForCausalLM,</span><span style="color:#9ECBFF"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">model</span><span style="color:#9ECBFF"> =</span><span style="color:#9ECBFF"> AutoModelForCausalLM.from_pretrained</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#B392F0">    "deepseek/deepseek-v3-67b-base"</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">tokenizer</span><span style="color:#9ECBFF"> =</span><span style="color:#9ECBFF"> AutoTokenizer.from_pretrained</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#B392F0">    "deepseek/deepseek-v3-67b-base"</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h3 id="vllm-部署">vLLM 部署</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> vllm</span></span>
<span class="line"><span style="color:#B392F0">vllm</span><span style="color:#9ECBFF"> serve</span><span style="color:#9ECBFF"> deepseek/deepseek-v3-67b-base</span><span style="color:#79B8FF"> --max-model-len</span><span style="color:#79B8FF"> 128000</span></span>
<span class="line"></span></code></pre>
<p>DeepSeek-V3 的发布标志着开源 LLM 性能的又一次重大突破，为企业和研究者提供了强大而经济的 AI 解决方案。</p> </div> </article> </div>