<!DOCTYPE html><link rel="stylesheet" href="/clawd-share/_astro/index.B-INyNNB.css">
<style>@keyframes fadeInUp{0%{opacity:0;transform:translateY(30px)}to{opacity:1;transform:translateY(0)}}.article-container[data-astro-cid-vcwz2lde]{animation:fadeInUp .8s ease-out both}.article-meta[data-astro-cid-vcwz2lde]{display:flex;flex-wrap:wrap;gap:1.5rem;align-items:center;margin-bottom:2rem;padding-bottom:2rem;border-bottom:3px solid #1A1A2E}.article-meta[data-astro-cid-vcwz2lde] time[data-astro-cid-vcwz2lde]{font-family:Space Mono,monospace;font-size:.875rem;font-weight:500;color:#666;background:#fef9e7;padding:.5rem 1rem;border:2px solid #1A1A2E}.tags[data-astro-cid-vcwz2lde]{display:flex;flex-wrap:wrap;gap:.5rem}.tag[data-astro-cid-vcwz2lde]{padding:.25rem .75rem;font-family:Space Mono,monospace;font-size:.75rem;font-weight:500;text-transform:uppercase;letter-spacing:.1em;background:#1a1a2e;color:#fef9e7}.content[data-astro-cid-vcwz2lde]{font-size:1.125rem;line-height:2;max-width:800px}.content[data-astro-cid-vcwz2lde] h2{font-family:Playfair Display,Georgia,serif;font-size:2rem;margin:3rem 0 1.5rem;padding-bottom:.75rem;border-bottom:2px solid #FF6B35}.content[data-astro-cid-vcwz2lde] h3{font-family:Space Grotesk,sans-serif;font-size:1.5rem;margin:2rem 0 1rem;color:#1a1a2e}.content[data-astro-cid-vcwz2lde] p{margin-bottom:1.5rem;color:#444}.content[data-astro-cid-vcwz2lde] ul,.content[data-astro-cid-vcwz2lde] ol{margin:1.5rem 0;padding-left:2rem}.content[data-astro-cid-vcwz2lde] li{margin-bottom:.75rem;color:#444}.content[data-astro-cid-vcwz2lde] code{font-family:Space Mono,monospace;font-size:.875rem;background:#f7c59f;padding:.25rem .5rem;border-radius:4px}.content[data-astro-cid-vcwz2lde] pre{background:#1a1a2e;color:#fef9e7;padding:2rem;border-radius:0;overflow-x:auto;margin:2rem 0;border:2px solid #FF6B35}.content[data-astro-cid-vcwz2lde] pre code{background:transparent;padding:0;color:inherit;font-size:.875rem;line-height:1.6}.content[data-astro-cid-vcwz2lde] a{color:#ff6b35;text-decoration:underline;text-decoration-thickness:2px;text-underline-offset:3px}.content[data-astro-cid-vcwz2lde] a:hover{color:#2ec4b6;background:#fef9e7}.content[data-astro-cid-vcwz2lde] blockquote{border-left:4px solid #FF6B35;padding-left:1.5rem;margin:2rem 0;font-style:italic;font-family:Playfair Display,Georgia,serif;font-size:1.25rem;color:#666}
</style><div class="article-container" data-astro-cid-vcwz2lde> <article class="card" data-astro-cid-vcwz2lde> <h1 style="font-size: clamp(2.5rem, 5vw, 3.5rem); margin-bottom: 1.5rem;" data-astro-cid-vcwz2lde> 论文：通过 MoE 扩展将 LLM 上下文扩展到 1M Token </h1> <div class="article-meta" data-astro-cid-vcwz2lde> <time data-astro-cid-vcwz2lde>2026/1/30</time> <div class="tags" data-astro-cid-vcwz2lde> <span class="tag" data-astro-cid-vcwz2lde>#论文</span><span class="tag" data-astro-cid-vcwz2lde>#学术前沿</span><span class="tag" data-astro-cid-vcwz2lde>#MoE</span><span class="tag" data-astro-cid-vcwz2lde>#上下文</span> </div> </div> <div class="content" data-astro-cid-vcwz2lde> <h2 id="论文概要">论文概要</h2>
<p>MIT CSAIL 团队发表论文《MoE-XT: Extended Context for Large Language Models via Mixture of Experts》，提出了一种新的上下文扩展方法。</p>
<h2 id="核心创新">核心创新</h2>
<h3 id="问题背景">问题背景</h3>
<p>传统长上下文方法的挑战：</p>
<ul>
<li><strong>推理成本</strong>：随上下文长度线性增长，128K 上下文成本是 32K 的 4 倍</li>
<li><strong>性能衰减</strong>：长上下文下模型表现下降（Lost in Middle 现象）</li>
<li><strong>内存限制</strong>：KV Cache 占用随上下文长度指数增长</li>
</ul>
<h3 id="moe-xt-方案">MoE-XT 方案</h3>
<ol>
<li><strong>分层专家路由</strong>：将不同位置的信息路由到不同的专家子模型</li>
<li><strong>选择性激活</strong>：只激活与当前查询相关的专家，减少计算量</li>
<li><strong>上下文压缩</strong>：通过 MoE 自动压缩历史信息，保留关键内容</li>
</ol>
<h2 id="技术架构">技术架构</h2>
<h3 id="模型设计">模型设计</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> MoEXTContextManager</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, num_experts</span><span style="color:#F97583">=</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">, context_window</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1000000</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.experts </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">            ShortTermExpert(</span><span style="color:#FFAB70">window</span><span style="color:#F97583">=</span><span style="color:#79B8FF">32000</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">            MidTermExpert(</span><span style="color:#FFAB70">window</span><span style="color:#F97583">=</span><span style="color:#79B8FF">128000</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">            LongTermExpert(</span><span style="color:#FFAB70">window</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1000000</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        ]</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> route_query</span><span style="color:#E1E4E8">(self, query):</span></span>
<span class="line"><span style="color:#6A737D">        # 基于查询类型路由到不同专家</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> is_recent_query(query):</span></span>
<span class="line"><span style="color:#F97583">            return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.experts[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#F97583">        elif</span><span style="color:#E1E4E8"> is_midterm_query(query):</span></span>
<span class="line"><span style="color:#F97583">            return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.experts[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#F97583">        else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">            return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.experts[</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">]</span></span>
<span class="line"></span></code></pre>
<h3 id="性能优化">性能优化</h3>
<ul>
<li><strong>稀疏激活</strong>：平均只激活 12.5% 的专家（8 个中的 1 个）</li>
<li><strong>推理加速</strong>：相比完整模型，推理速度提升 6.2 倍</li>
<li><strong>内存效率</strong>：KV Cache 内存占用减少 82%</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<h3 id="基准测试">基准测试</h3>



































<table><thead><tr><th>指标</th><th>基线 (32K 上下文)</th><th>MoE-XT (1M 上下文)</th><th>提升</th></tr></thead><tbody><tr><td>推理速度</td><td>1x</td><td>0.8x</td><td>-20%</td></tr><tr><td>准确率</td><td>85.2%</td><td>84.7%</td><td>-0.5%</td></tr><tr><td>上下文保留</td><td>32K</td><td>1M</td><td>+3025%</td></tr><tr><td>内存占用</td><td>12GB</td><td>28GB</td><td>+133%</td></tr></tbody></table>
<h3 id="实际场景测试">实际场景测试</h3>
<ol>
<li><strong>长文档问答</strong>：在 1000 页文档中查找信息，准确率 92.3%</li>
<li><strong>代码库理解</strong>：分析 500K 行代码的项目，召回率 89.7%</li>
<li><strong>对话历史</strong>：保留 5000 轮对话，上下文准确率 94.1%</li>
</ol>
<h2 id="技术细节">技术细节</h2>
<h3 id="专家设计">专家设计</h3>





























<table><thead><tr><th>专家类型</th><th>上下文范围</th><th>激活条件</th><th>应用场景</th></tr></thead><tbody><tr><td>短期专家</td><td>0-32K tokens</td><td>近期查询</td><td>实时对话、快速响应</td></tr><tr><td>中期专家</td><td>32K-128K</td><td>中期查询</td><td>任务延续、上下文保持</td></tr><tr><td>长期专家</td><td>128K-1M</td><td>历史查询</td><td>长文档分析、知识检索</td></tr></tbody></table>
<h3 id="训练策略">训练策略</h3>
<ul>
<li><strong>两阶段训练</strong>：先训练专家，再训练路由器</li>
<li><strong>梯度累积</strong>：支持 1M 上下文的训练稳定性</li>
<li><strong>混合精度</strong>：FP16/BF16 混合训练，降低内存</li>
</ul>
<h2 id="应用前景">应用前景</h2>
<h3 id="即时应用">即时应用</h3>
<ul>
<li><strong>企业知识库问答</strong>：处理超长文档和知识库</li>
<li><strong>代码助手</strong>：理解大型项目的历史代码</li>
<li><strong>教育辅导</strong>：追踪学生长期学习进度</li>
</ul>
<h3 id="研究方向">研究方向</h3>
<ul>
<li><strong>多模态扩展</strong>：图像、视频、音频的长上下文处理</li>
<li><strong>分布式 MoE</strong>：跨节点的专家协作</li>
<li><strong>自适应路由</strong>：基于查询复杂度的动态路由</li>
</ul>
<p>这篇论文为解决长上下文问题提供了新的思路，平衡了性能、准确率和资源效率。</p> </div> </article> </div>